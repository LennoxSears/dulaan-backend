<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real API Test Runner</title>
    <style>
        body {
            font-family: 'Courier New', monospace;
            background: #1a1a1a;
            color: #00ff00;
            padding: 20px;
            line-height: 1.4;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        .test-output {
            background: #000;
            border: 1px solid #333;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            white-space: pre-wrap;
            font-size: 14px;
            max-height: 600px;
            overflow-y: auto;
        }
        .button {
            background: #333;
            color: #00ff00;
            border: 1px solid #555;
            padding: 10px 20px;
            cursor: pointer;
            margin: 10px 5px;
            border-radius: 3px;
        }
        .button:hover {
            background: #555;
        }
        .button:disabled {
            background: #222;
            color: #666;
            cursor: not-allowed;
        }
        .success { color: #00ff00; }
        .error { color: #ff4444; }
        .warning { color: #ffaa00; }
        .info { color: #4488ff; }
        h1, h2 { color: #ffffff; }
        .stats {
            background: #222;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .vad-status {
            background: #1a1a3a;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            border-left: 3px solid #4488ff;
        }
        .vad-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
            background: #666;
        }
        .vad-indicator.listening {
            background: #00ff00;
            animation: pulse 1s infinite;
        }
        .vad-indicator.processing {
            background: #ffaa00;
            animation: spin 1s linear infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        @keyframes spin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        .api-info {
            background: #1a3a1a;
            padding: 10px;
            border-radius: 3px;
            margin: 10px 0;
            border-left: 3px solid #00ff00;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ Real API Test Runner</h1>
        <p>Testing actual Firebase speechToTextWithLLM function with multi-language audio samples</p>
        
        <div class="api-info">
            <strong>ğŸ”— API Endpoint:</strong> https://directaudiotopwm-qveg3gkwxa-ew.a.run.app<br>
            <strong>ğŸ¯ Test Type:</strong> Real Google Speech-to-Text + Gemini LLM<br>
            <strong>ğŸ“Š Audio Format:</strong> Int16Array (LINEAR16 PCM, 16kHz) - New Efficient Format
        </div>
        
        <div class="stats">
            <h2>Test Configuration</h2>
            <div id="config-info">
                <div>ğŸ‡ºğŸ‡¸ English: "Turn it up" (~1.2s)</div>
                <div>ğŸ‡ªğŸ‡¸ Spanish: "MÃ¡s fuerte" (~1.3s)</div>
                <div>ğŸ‡¨ğŸ‡³ Chinese: "åŠ å¼º" (~1.1s)</div>
                <div>âš¡ Expected: All commands should increase motor PWM</div>
            </div>
        </div>
        
        <button class="button" onclick="runRealApiTests()" id="runAllBtn">ğŸš€ Test Real API (All Languages)</button>
        <button class="button" onclick="testSingleLanguage('english')" id="testEnBtn">ğŸ‡ºğŸ‡¸ Test English</button>
        <button class="button" onclick="testSingleLanguage('spanish')" id="testEsBtn">ğŸ‡ªğŸ‡¸ Test Spanish</button>
        <button class="button" onclick="testSingleLanguage('chinese')" id="testZhBtn">ğŸ‡¨ğŸ‡³ Test Chinese</button>
        <button class="button" onclick="testApiConnectivity()" id="testConnBtn">ğŸ”§ Test API Connectivity</button>
        <button class="button" onclick="startOptimizedVoiceControl()" id="startVoiceBtn">ğŸ¤ Start Natural Voice Control (VAD)</button>
        <button class="button" onclick="stopOptimizedVoiceControl()" id="stopVoiceBtn" disabled>ğŸ”‡ Stop Voice Control</button>
        <button class="button" onclick="testLLMOnly()" id="testLLMBtn">ğŸ¤– Test LLM Only</button>
        <button class="button" onclick="clearOutput()">ğŸ—‘ï¸ Clear Output</button>
        
        <div id="test-output" class="test-output">
Ready to test real API...\n\nClick "Test Real API" to start testing with direct audio-to-PWM processing via Gemini 2.0.

âš ï¸  Note: This will make real API calls to Firebase functions.
        </div>
        
        <div id="vad-status" class="vad-status" style="display: none;">
            <h2>ğŸ™ï¸ Optimized Voice Control Status</h2>
            <div id="vad-content">
                <div style="margin-bottom: 10px;">
                    <span id="vad-indicator" class="vad-indicator"></span>
                    <span id="vad-text" style="font-weight: bold;">Ready</span>
                </div>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                    <div>
                        <div><strong>ğŸ›ï¸ Motor Control:</strong></div>
                        <div>Current PWM: <span id="current-pwm" style="color: #00ff00; font-weight: bold;">100</span></div>
                        <div>Last Command: <span id="last-command">None</span></div>
                    </div>
                    <div>
                        <div><strong>ğŸ“Š Efficiency Stats:</strong></div>
                        <div>API Reduction: <span id="efficiency-display" style="color: #00ff00; font-weight: bold;">0%</span></div>
                        <div>Chunks: <span id="total-chunks">0</span> | Calls: <span id="api-calls">0</span></div>
                    </div>
                </div>
                <div style="margin-top: 10px;">
                    <div><strong>ğŸ”Š Audio Analysis:</strong></div>
                    <div>RMS Energy: <span id="rms-energy">0.000</span> | ZCR: <span id="zcr-rate">0.000</span></div>
                    <div>Speech Buffer: <span id="speech-buffer-size">0</span> samples | VAD Buffer: <span id="vad-buffer-size">0</span> samples</div>
                </div>
            </div>
        </div>

        <div id="efficiency-comparison" class="stats" style="display: none;">
            <h2>ğŸ“ˆ Efficiency Comparison</h2>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                <div style="background: #3a1a1a; padding: 15px; border-radius: 5px; border-left: 3px solid #ff4444;">
                    <h3 style="color: #ff4444; margin-top: 0;">âŒ Traditional Streaming</h3>
                    <div>API Calls: <span id="traditional-calls">0</span></div>
                    <div>Bandwidth: <span id="traditional-bandwidth">0 KB</span></div>
                    <div>Efficiency: <span style="color: #ff4444;">0%</span></div>
                </div>
                <div style="background: #1a3a1a; padding: 15px; border-radius: 5px; border-left: 3px solid #00ff00;">
                    <h3 style="color: #00ff00; margin-top: 0;">âœ… Optimized VAD</h3>
                    <div>API Calls: <span id="optimized-calls">0</span></div>
                    <div>Bandwidth: <span id="optimized-bandwidth">0 KB</span></div>
                    <div>Efficiency: <span id="optimized-efficiency" style="color: #00ff00;">0%</span></div>
                </div>
            </div>
            <div style="margin-top: 15px; text-align: center;">
                <strong>ğŸ’° Cost Savings: <span id="cost-savings" style="color: #00ff00;">$0.00</span> per hour</strong>
            </div>
        </div>

        <div id="test-stats" class="stats" style="display: none;">
            <h2>ğŸ“Š Real API Test Results</h2>
            <div id="stats-content"></div>
        </div>
    </div>

    <script type="module">
        const API_URL = 'https://directaudiotopwm-qveg3gkwxa-ew.a.run.app';
        
        // Ring Buffer implementation for efficient audio buffering
        class RingBuffer {
            constructor(capacity) {
                this.capacity = capacity;
                this.buffer = new Float32Array(capacity);
                this.writeIndex = 0;
                this.count = 0;
            }

            push(data) {
                if (Array.isArray(data) || data instanceof Float32Array || data instanceof Int16Array) {
                    for (let i = 0; i < data.length; i++) {
                        this.pushSingle(data[i]);
                    }
                } else {
                    this.pushSingle(data);
                }
            }

            pushSingle(value) {
                this.buffer[this.writeIndex] = value;
                this.writeIndex = (this.writeIndex + 1) % this.capacity;
                this.count = Math.min(this.count + 1, this.capacity);
            }

            readLast(samples) {
                const result = new Float32Array(Math.min(samples, this.count));
                let readIndex = (this.writeIndex - Math.min(samples, this.count) + this.capacity) % this.capacity;
                
                for (let i = 0; i < result.length; i++) {
                    result[i] = this.buffer[readIndex];
                    readIndex = (readIndex + 1) % this.capacity;
                }
                
                return result;
            }

            readAll() {
                return this.readLast(this.count);
            }

            reset() {
                this.writeIndex = 0;
                this.count = 0;
            }
        }
        
        // Test output management
        function log(message, type = 'info') {
            const output = document.getElementById('test-output');
            const timestamp = new Date().toLocaleTimeString();
            const colorClass = type === 'error' ? 'error' : type === 'success' ? 'success' : type === 'warning' ? 'warning' : 'info';
            output.innerHTML += `<span class="${colorClass}">[${timestamp}] ${message}</span>\n`;
            output.scrollTop = output.scrollHeight;
        }
        
        function clearOutput() {
            document.getElementById('test-output').innerHTML = 'Output cleared.\n';
            document.getElementById('test-stats').style.display = 'none';
        }
        
        function setButtonsEnabled(enabled) {
            const buttons = ['runAllBtn', 'testEnBtn', 'testEsBtn', 'testZhBtn', 'testConnBtn', 'startVoiceBtn', 'testLLMBtn'];
            buttons.forEach(id => {
                document.getElementById(id).disabled = !enabled;
            });
        }

        // Optimized VAD System with Ring Buffers
        class OptimizedVADProcessor {
            constructor() {
                this.isActive = false;
                this.isListening = false;
                this.isProcessing = false;
                this.currentPwm = 100;
                
                // Ring buffers for efficient memory usage
                this.vadBuffer = new RingBuffer(1600); // 100ms for VAD analysis
                this.speechBuffer = new RingBuffer(16000 * 10); // 10 seconds max speech
                
                // VAD state
                this.consecutiveVoiceFrames = 0;
                this.consecutiveSilenceFrames = 0;
                this.isVoiceActive = false;
                this.voiceStartTime = 0;
                
                // Efficiency tracking
                this.totalChunks = 0;
                this.apiCalls = 0;
                this.lastRMS = 0;
                this.lastZeroCrossings = 0;
                
                // Audio processing
                this.audioContext = null;
                this.mediaStream = null;
                this.processor = null;
                
                // Optimized thresholds
                this.VAD_ENERGY_THRESHOLD = 0.015;
                this.VAD_ZCR_THRESHOLD = 0.12;
                this.VAD_VOICE_FRAMES = 2; // Faster voice detection
                this.VAD_SILENCE_FRAMES = 15; // Longer silence confirmation
                this.MIN_SPEECH_DURATION = 8000; // 500ms minimum (in samples)
                this.MAX_SPEECH_DURATION = 80000; // 5 seconds maximum
            }

            async start() {
                try {
                    log("ğŸ¤ Starting optimized voice control with VAD...", 'info');
                    
                    // Request microphone access
                    this.mediaStream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            sampleRate: 16000,
                            channelCount: 1,
                            echoCancellation: true,
                            noiseSuppression: true
                        }
                    });
                    
                    // Create audio context
                    this.audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                    const source = this.audioContext.createMediaStreamSource(this.mediaStream);
                    this.processor = this.audioContext.createScriptProcessor(1024, 1, 1);
                    
                    // Process audio chunks
                    this.processor.onaudioprocess = (e) => {
                        if (this.isActive) {
                            this.processAudioChunk(e.inputBuffer.getChannelData(0));
                        }
                    };
                    
                    source.connect(this.processor);
                    this.processor.connect(this.audioContext.destination);
                    
                    this.isActive = true;
                    this.updateVADStatus('Ready - speak naturally', false, false);
                    
                    log("âœ… Voice control started! Speak naturally - no buttons needed.", 'success');
                    log("ğŸ¯ Say commands like: 'turn it up', 'mÃ¡s fuerte', 'åŠ å¼º'", 'info');
                    
                    document.getElementById('vad-status').style.display = 'block';
                    document.getElementById('startVoiceBtn').disabled = true;
                    document.getElementById('stopVoiceBtn').disabled = false;
                    
                    return true;
                    
                } catch (error) {
                    log(`âŒ Failed to start voice control: ${error.message}`, 'error');
                    return false;
                }
            }

            stop() {
                log("ğŸ”‡ Stopping voice control...", 'info');
                
                this.isActive = false;
                this.isListening = false;
                this.isProcessing = false;
                
                if (this.mediaStream) {
                    this.mediaStream.getTracks().forEach(track => track.stop());
                    this.mediaStream = null;
                }
                
                if (this.processor) {
                    this.processor.disconnect();
                    this.processor = null;
                }
                
                if (this.audioContext) {
                    this.audioContext.close();
                    this.audioContext = null;
                }
                
                this.updateVADStatus('Stopped', false, false);
                
                document.getElementById('startVoiceBtn').disabled = false;
                document.getElementById('stopVoiceBtn').disabled = true;
                
                log("âœ… Voice control stopped", 'success');
            }

            processAudioChunk(audioData) {
                this.totalChunks++;
                
                // Always buffer audio for pre/post-speech context (smart buffering)
                this.vadBuffer.push(audioData);
                
                // Advanced Voice Activity Detection
                const isVoiceActive = this.detectVoiceActivity(audioData);
                
                if (isVoiceActive) {
                    this.consecutiveVoiceFrames++;
                    this.consecutiveSilenceFrames = 0;
                    
                    // Voice start detection
                    if (!this.isVoiceActive && this.consecutiveVoiceFrames >= this.VAD_VOICE_FRAMES) {
                        this.handleVoiceStart();
                    }
                    
                    // Buffer speech audio during active speech
                    if (this.isVoiceActive) {
                        this.speechBuffer.push(audioData);
                        this.checkSpeechBufferLimits();
                    }
                    
                } else {
                    this.consecutiveSilenceFrames++;
                    this.consecutiveVoiceFrames = 0;
                    
                    // Voice end detection
                    if (this.isVoiceActive && this.consecutiveSilenceFrames >= this.VAD_SILENCE_FRAMES) {
                        this.handleVoiceEnd();
                    }
                }
                
                // Update efficiency display
                this.updateEfficiencyDisplay();
            }

            detectVoiceActivity(audioData) {
                // Calculate RMS energy
                const rms = this.calculateRMS(audioData);
                this.lastRMS = rms;
                
                // Calculate zero crossing rate
                const zcr = this.calculateZeroCrossingRate(audioData);
                this.lastZeroCrossings = zcr;
                
                // Combined VAD decision
                const energyActive = rms > this.VAD_ENERGY_THRESHOLD;
                const zcrActive = zcr > this.VAD_ZCR_THRESHOLD;
                
                // Voice is active if both energy and ZCR indicate speech
                return energyActive && zcrActive;
            }

            calculateRMS(audioData) {
                let sum = 0;
                for (let i = 0; i < audioData.length; i++) {
                    sum += audioData[i] * audioData[i];
                }
                return Math.sqrt(sum / audioData.length);
            }

            calculateZeroCrossingRate(audioData) {
                let crossings = 0;
                for (let i = 1; i < audioData.length; i++) {
                    if ((audioData[i] >= 0) !== (audioData[i - 1] >= 0)) {
                        crossings++;
                    }
                }
                return crossings / audioData.length;
            }

            checkSpeechBufferLimits() {
                const speechDuration = Date.now() - this.voiceStartTime;
                const bufferSize = this.speechBuffer.count;
                
                // Send if max duration reached or buffer is full
                if (speechDuration >= (this.MAX_SPEECH_DURATION / 16000 * 1000) || 
                    bufferSize >= this.speechBuffer.capacity * 0.9) {
                    
                    log("ğŸ“¦ Buffer limit reached - sending speech chunk", 'warning');
                    this.processSpeech(false); // Not final
                    
                    // Keep some overlap for continuity
                    const overlapSize = Math.min(3200, bufferSize * 0.1); // 200ms overlap
                    const overlapData = this.speechBuffer.readLast(overlapSize);
                    this.speechBuffer.reset();
                    if (overlapData.length > 0) {
                        this.speechBuffer.push(overlapData);
                    }
                }
            }



            handleVoiceStart() {
                this.isVoiceActive = true;
                this.isListening = true;
                this.voiceStartTime = Date.now();
                
                // Smart buffering: Include pre-speech context for natural start
                this.speechBuffer.reset();
                
                // Add recent VAD buffer content as pre-speech context (last 200ms)
                const preSpeechSamples = Math.min(3200, this.vadBuffer.count); // 200ms at 16kHz
                if (preSpeechSamples > 0) {
                    const preSpeechData = this.vadBuffer.readLast(preSpeechSamples);
                    this.speechBuffer.push(preSpeechData);
                    log(`ğŸ™ï¸ Voice start - added ${preSpeechSamples} pre-speech samples`, 'info');
                }
                
                this.updateVADStatus('Listening...', true, false);
                log("ğŸ™ï¸ Voice detected - listening...", 'info');
            }

            async handleVoiceEnd() {
                // Add small post-speech buffer for natural ending (100ms)
                const postSpeechDelay = 100;
                
                setTimeout(async () => {
                    this.isVoiceActive = false;
                    this.isListening = false;
                    const speechDuration = Date.now() - this.voiceStartTime;
                    
                    // Add recent VAD buffer as post-speech context
                    const postSpeechSamples = Math.min(1600, this.vadBuffer.count); // 100ms
                    if (postSpeechSamples > 0) {
                        const postSpeechData = this.vadBuffer.readLast(postSpeechSamples);
                        this.speechBuffer.push(postSpeechData);
                        log(`ğŸ”‡ Voice end - added ${postSpeechSamples} post-speech samples`, 'info');
                    }
                    
                    log(`ğŸ”‡ Speech duration: ${speechDuration}ms, Buffer: ${this.speechBuffer.count} samples`);
                    
                    // Send speech to API if we have enough audio
                    if (this.speechBuffer.count >= this.MIN_SPEECH_DURATION) {
                        await this.processSpeech(true); // Mark as final
                    } else {
                        log("ğŸ”‡ Speech too short, discarding", 'warning');
                        this.speechBuffer.reset();
                        this.updateVADStatus('Ready - speak naturally', false, false);
                    }
                }, postSpeechDelay);
            }

            async processSpeech(isFinal = true) {
                this.isProcessing = true;
                this.apiCalls++;
                
                this.updateVADStatus('Processing speech...', false, true);
                
                try {
                    const speechData = this.speechBuffer.readAll();
                    if (speechData.length === 0) return null;

                    // Convert to Int16Array for API
                    const int16Data = new Int16Array(speechData.length);
                    for (let i = 0; i < speechData.length; i++) {
                        const scaled = Math.max(-1, Math.min(1, speechData[i])) * 32767;
                        int16Data[i] = Math.max(-32768, Math.min(32767, scaled));
                    }

                    const speechDuration = (speechData.length / 16000).toFixed(2);
                    log(`âš¡ Processing ${speechDuration}s of speech (API call #${this.apiCalls})`, 'info');
                    
                    // Call real API with speech data
                    const result = await callRealApi(
                        Array.from(int16Data),
                        this.currentPwm,
                        [],
                        'auto',
                        { language: 'Auto-detected', command: 'Natural voice input' }
                    );
                    
                    // Update PWM if changed
                    if (result.newPwmValue !== undefined) {
                        this.currentPwm = result.newPwmValue;
                        document.getElementById('current-pwm').textContent = this.currentPwm;
                    }
                    
                    // Log results
                    log(`âœ… Speech processed successfully!`, 'success');
                    log(`ğŸ¤ "${result.transcription || 'No transcription'}"`, 'info');
                    log(`ğŸ¤– "${result.response || 'No response'}"`, 'info');
                    log(`âš¡ PWM: ${result.previousPwm || this.currentPwm} â†’ ${result.newPwmValue || this.currentPwm}`, 'info');
                    
                    // Update last command display
                    if (result.transcription) {
                        document.getElementById('last-command').textContent = result.transcription;
                    }
                    
                    if (result.intentDetected) {
                        log(`ğŸ¯ Motor command detected and executed!`, 'success');
                    } else {
                        log(`ğŸ’¬ Conversational response (no motor action)`, 'info');
                    }

                    // Reset buffer after sending if final
                    if (isFinal) {
                        this.speechBuffer.reset();
                    }

                    return result;
                    
                } catch (error) {
                    log(`âŒ Speech processing failed: ${error.message}`, 'error');
                    throw error;
                } finally {
                    this.isProcessing = false;
                    if (isFinal) {
                        this.updateVADStatus('Ready - speak naturally', false, false);
                    }
                }
            }

            updateVADStatus(text, listening, processing) {
                const indicator = document.getElementById('vad-indicator');
                const textElement = document.getElementById('vad-text');
                
                if (processing) {
                    indicator.className = 'vad-indicator processing';
                } else if (listening) {
                    indicator.className = 'vad-indicator listening';
                } else {
                    indicator.className = 'vad-indicator';
                }
                
                textElement.textContent = text;
            }

            updateEfficiencyComparison() {
                // Show comparison panel
                document.getElementById('efficiency-comparison').style.display = 'block';
                
                // Traditional approach would call API for every chunk
                const traditionalCalls = this.totalChunks;
                const optimizedCalls = this.apiCalls;
                
                // Estimate bandwidth (assuming 1KB per chunk for traditional, 10KB per speech segment for optimized)
                const traditionalBandwidth = (traditionalCalls * 1).toFixed(1);
                const optimizedBandwidth = (optimizedCalls * 10).toFixed(1);
                
                // Calculate efficiency
                const efficiency = this.totalChunks > 0 ? 
                    ((1 - this.apiCalls / this.totalChunks) * 100).toFixed(1) : 0;
                
                // Estimate cost savings (assuming $0.001 per API call)
                const costSavings = ((traditionalCalls - optimizedCalls) * 0.001 * 3600 / Math.max(1, this.totalChunks / 100)).toFixed(2);
                
                // Update display
                document.getElementById('traditional-calls').textContent = traditionalCalls;
                document.getElementById('traditional-bandwidth').textContent = traditionalBandwidth;
                document.getElementById('optimized-calls').textContent = optimizedCalls;
                document.getElementById('optimized-bandwidth').textContent = optimizedBandwidth;
                document.getElementById('optimized-efficiency').textContent = efficiency + '%';
                document.getElementById('cost-savings').textContent = '$' + costSavings;
            }

            updateEfficiencyDisplay() {
                const efficiency = this.totalChunks > 0 ? 
                    ((1 - this.apiCalls / this.totalChunks) * 100).toFixed(1) : 0;
                
                // Update efficiency stats
                document.getElementById('efficiency-display').textContent = efficiency + '%';
                document.getElementById('total-chunks').textContent = this.totalChunks;
                document.getElementById('api-calls').textContent = this.apiCalls;
                
                // Update audio analysis
                document.getElementById('rms-energy').textContent = this.lastRMS.toFixed(3);
                document.getElementById('zcr-rate').textContent = this.lastZeroCrossings.toFixed(3);
                document.getElementById('speech-buffer-size').textContent = this.speechBuffer.count;
                document.getElementById('vad-buffer-size').textContent = this.vadBuffer.count;
                
                // Color coding for efficiency
                const efficiencyElement = document.getElementById('efficiency-display');
                if (parseFloat(efficiency) > 90) {
                    efficiencyElement.style.color = '#00ff00'; // Green for excellent
                } else if (parseFloat(efficiency) > 70) {
                    efficiencyElement.style.color = '#ffaa00'; // Orange for good
                } else {
                    efficiencyElement.style.color = '#ff4444'; // Red for poor
                }
                
                // Update efficiency comparison
                this.updateEfficiencyComparison();
            }
        }

        // Global VAD processor instance
        let vadProcessor = null;

        // Start optimized voice control
        async function startOptimizedVoiceControl() {
            if (!vadProcessor) {
                vadProcessor = new OptimizedVADProcessor();
            }
            
            const success = await vadProcessor.start();
            if (success) {
                setButtonsEnabled(false);
                document.getElementById('stopVoiceBtn').disabled = false;
            }
        }

        // Stop optimized voice control
        function stopOptimizedVoiceControl() {
            if (vadProcessor) {
                vadProcessor.stop();
                setButtonsEnabled(true);
                document.getElementById('stopVoiceBtn').disabled = true;
            }
        }
        
        // Test API connectivity with minimal request
        async function testApiConnectivity() {
            setButtonsEnabled(false);
            
            log(`\nğŸ”§ Testing API Connectivity...`);
            log(`ğŸ“¡ Endpoint: ${API_URL}`);
            
            try {
                // Test with minimal valid request
                const testRequest = {
                    msgHis: [],
                    audioBuffer: [0, 100, 0, -100, 0], // Minimal test audio
                    currentPwm: 100,
                    encoding: 'LINEAR16',
                    sampleRateHertz: 16000,
                    languageCode: 'en-US'
                };
                
                log(`ğŸ“¦ Sending minimal test request...`);
                
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(testRequest)
                });
                
                log(`ğŸ“Š Response Status: ${response.status} ${response.statusText}`);
                
                if (response.ok) {
                    const result = await response.json();
                    log(`âœ… API is responding!`, 'success');
                    log(`ğŸ“„ Response structure: ${JSON.stringify(Object.keys(result))}`);
                    log(`ğŸ” Success field: ${result.success}`);
                    log(`ğŸ” Error field: ${result.error || 'none'}`);
                } else {
                    const errorText = await response.text();
                    log(`âŒ API error: ${errorText}`, 'error');
                }
                
            } catch (error) {
                log(`âŒ Connection failed: ${error.message}`, 'error');
            } finally {
                setButtonsEnabled(true);
            }
        }
        
        // Test with real microphone input
        async function testWithMicrophone() {
            setButtonsEnabled(false);
            
            log(`\nğŸ¤ Testing with Real Microphone...`);
            
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                log(`âœ… Microphone access granted`, 'success');
                log(`ğŸ™ï¸ Please say a motor control command in any language:`, 'info');
                log(`   ğŸ‡ºğŸ‡¸ English: "turn it up" / "make it stronger"`);
                log(`   ğŸ‡ªğŸ‡¸ Spanish: "mÃ¡s fuerte" / "aumenta la potencia"`);
                log(`   ğŸ‡¨ğŸ‡³ Chinese: "åŠ å¼º" / "å¢åŠ åŠŸç‡"`);
                log(`   ğŸŒ Or any motor control command in your language...`);
                
                // Create audio context and recorder
                const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(stream);
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                const audioData = [];
                let recording = true;
                
                processor.onaudioprocess = function(e) {
                    if (recording) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        // Convert to Int16Array
                        for (let i = 0; i < inputData.length; i++) {
                            const sample = Math.max(-1, Math.min(1, inputData[i]));
                            audioData.push(Math.round(sample * 32767));
                        }
                    }
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                // Countdown for user
                let countdown = 3;
                const countdownInterval = setInterval(() => {
                    if (countdown > 0) {
                        log(`ğŸ™ï¸ Recording in ${countdown}...`);
                        countdown--;
                    } else {
                        log(`ğŸ”´ Recording now! Speak your command...`, 'success');
                        clearInterval(countdownInterval);
                    }
                }, 1000);
                
                // Record for 4 seconds (1 second countdown + 3 seconds recording)
                setTimeout(async () => {
                    recording = false;
                    stream.getTracks().forEach(track => track.stop());
                    processor.disconnect();
                    source.disconnect();
                    
                    log(`ğŸµ Recorded ${audioData.length} samples (${(audioData.length / 16000).toFixed(2)}s)`);
                    
                    // Test with recorded audio using auto language detection
                    try {
                        const result = await callRealApi(
                            audioData,
                            100,
                            [],
                            'auto', // Use auto-detection instead of hardcoded language
                            { language: 'Auto-detected', command: 'Real microphone input' }
                        );
                        
                        log(`\nğŸ“‹ Real Microphone Test Results:`);
                        log(`   - Success: ${result.success ? 'âœ…' : 'âŒ'}`);
                        log(`   - Transcription: "${result.transcription || 'None'}"`);
                        log(`   - Detected Language: ${result.detectedLanguage || 'Unknown'}`);
                        log(`   - Confidence: ${result.confidence ? (result.confidence * 100).toFixed(1) + '%' : 'N/A'}`);
                        log(`   - Intent Detected: ${result.intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM Change: ${result.previousPwm || 100} â†’ ${result.newPwmValue || 100}`);
                        log(`   - Response: "${result.response || 'None'}"`);
                        
                        // Validate results
                        const hasTranscription = result.transcription && result.transcription.length > 0;
                        const hasLanguageDetection = result.detectedLanguage && result.detectedLanguage.length > 0;
                        const intentDetected = result.intentDetected;
                        const pwmChanged = result.newPwmValue !== (result.previousPwm || 100);
                        const overallSuccess = result.success && hasTranscription && intentDetected;
                        
                        log(`\nğŸ“Š Microphone Test Validation:`);
                        log(`   - Has transcription: ${hasTranscription ? 'âœ…' : 'âŒ'}`);
                        log(`   - Language detected: ${hasLanguageDetection ? 'âœ…' : 'âŒ'}`);
                        log(`   - Intent recognized: ${intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM changed: ${pwmChanged ? 'âœ…' : 'âŒ'}`);
                        log(`   - Overall result: ${overallSuccess ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, 
                            overallSuccess ? 'success' : 'error');
                        
                        if (!overallSuccess && hasTranscription) {
                            log(`\nğŸ’¡ Tip: Try saying clearer motor control commands like:`, 'warning');
                            log(`   ğŸ‡ºğŸ‡¸ "turn it up", "make it stronger", "increase power"`);
                            log(`   ğŸ‡ªğŸ‡¸ "mÃ¡s fuerte", "aumenta la potencia"`);
                            log(`   ğŸ‡¨ğŸ‡³ "åŠ å¼º", "å¢åŠ åŠŸç‡", "å¼€å¤§ä¸€ç‚¹"`);
                        }
                        
                    } catch (error) {
                        log(`âŒ Real microphone test failed: ${error.message}`, 'error');
                    }
                    
                    setButtonsEnabled(true);
                }, 4000); // 1 second countdown + 3 seconds recording
                
            } catch (error) {
                log(`âŒ Microphone access failed: ${error.message}`, 'error');
                setButtonsEnabled(true);
            }
        }
        
        // Test LLM processing only (bypass speech recognition)
        async function testLLMOnly() {
            setButtonsEnabled(false);
            
            log(`\nğŸ¤– Testing LLM Processing Only...`);
            log(`This bypasses speech recognition and tests the motor control logic directly.`);
            
            const testCases = [
                { transcript: 'turn it up', language: 'English', expected: 'increase' },
                { transcript: 'mÃ¡s fuerte', language: 'Spanish', expected: 'increase' },
                { transcript: 'åŠ å¼º', language: 'Chinese', expected: 'increase' }
            ];
            
            for (const testCase of testCases) {
                try {
                    log(`\nğŸ§ª Testing "${testCase.transcript}" (${testCase.language})`);
                    
                    const testRequest = {
                        msgHis: [],
                        testMode: true, // Enable test mode
                        testTranscript: testCase.transcript, // Provide transcript directly
                        currentPwm: 100,
                        encoding: 'LINEAR16',
                        sampleRateHertz: 16000,
                        languageCode: testCase.language === 'English' ? 'en-US' : 
                                     testCase.language === 'Spanish' ? 'es-ES' : 'zh-CN'
                    };
                    
                    log(`ğŸ“¡ Sending test request with transcript: "${testCase.transcript}"`);
                    
                    const response = await fetch(API_URL, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(testRequest)
                    });
                    
                    log(`ğŸ“Š Response Status: ${response.status} ${response.statusText}`);
                    
                    if (response.ok) {
                        const result = await response.json();
                        log(`âœ… LLM test successful!`, 'success');
                        log(`ğŸ¤ Transcript: "${result.transcription}"`);
                        log(`ğŸ¤– Response: "${result.response}"`);
                        log(`âš¡ PWM: ${result.previousPwm} â†’ ${result.newPwmValue}`);
                        log(`ğŸ¯ Intent detected: ${result.intentDetected}`);
                        log(`ğŸ“ˆ Confidence: ${(result.confidence * 100).toFixed(1)}%`);
                        
                        // Validate results
                        const pwmIncreased = result.newPwmValue > result.previousPwm;
                        const intentDetected = result.intentDetected;
                        const hasResponse = result.response && result.response.length > 0;
                        
                        log(`ğŸ“‹ Validation:`);
                        log(`   - Intent detected: ${intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM increased: ${pwmIncreased ? 'âœ…' : 'âŒ'}`);
                        log(`   - Has response: ${hasResponse ? 'âœ…' : 'âŒ'}`);
                        log(`   - Overall: ${(intentDetected && pwmIncreased && hasResponse) ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, 
                            (intentDetected && pwmIncreased && hasResponse) ? 'success' : 'error');
                        
                    } else {
                        const errorText = await response.text();
                        log(`âŒ LLM test failed: ${errorText}`, 'error');
                    }
                    
                } catch (error) {
                    log(`âŒ LLM test error: ${error.message}`, 'error');
                }
                
                // Small delay between tests
                await new Promise(resolve => setTimeout(resolve, 500));
            }
            
            log(`\nğŸ LLM-only testing completed!`, 'success');
            setButtonsEnabled(true);
        }
        
        // Generate test audio data - using white noise with speech-like characteristics
        function generateTestAudio(duration, language) {
            const sampleRate = 16000;
            const samples = Math.floor(duration * sampleRate);
            const audioData = new Array(samples);
            
            // Use more speech-like frequencies and patterns
            let baseFreq = 200; // Higher base frequency for better recognition
            let amplitude = 0.5; // Lower amplitude to avoid clipping
            
            switch (language) {
                case 'english':
                    baseFreq = 220; // A3 note
                    break;
                case 'spanish':
                    baseFreq = 196; // G3 note
                    break;
                case 'chinese':
                    baseFreq = 247; // B3 note
                    break;
            }
            
            for (let i = 0; i < samples; i++) {
                const t = i / sampleRate;
                
                // Create more realistic speech pattern with vowel-like formants
                let signal = 0;
                
                // Fundamental frequency
                signal += Math.sin(2 * Math.PI * baseFreq * t) * amplitude;
                
                // First formant (around 500-700 Hz for vowels)
                signal += Math.sin(2 * Math.PI * (baseFreq * 2.5) * t) * amplitude * 0.6;
                
                // Second formant (around 1000-1500 Hz for vowels)
                signal += Math.sin(2 * Math.PI * (baseFreq * 5) * t) * amplitude * 0.4;
                
                // Third formant (around 2000-3000 Hz)
                signal += Math.sin(2 * Math.PI * (baseFreq * 10) * t) * amplitude * 0.2;
                
                // Add speech-like modulation
                const modulation = 1 + 0.1 * Math.sin(2 * Math.PI * 5 * t); // 5 Hz modulation
                signal *= modulation;
                
                // Natural envelope with attack and decay
                let envelope = 1;
                const attackTime = 0.1; // 100ms attack
                const decayTime = 0.1;  // 100ms decay
                
                if (t < attackTime) {
                    envelope = t / attackTime;
                } else if (t > duration - decayTime) {
                    envelope = (duration - t) / decayTime;
                }
                
                signal *= envelope;
                
                // Add minimal noise for realism
                signal += (Math.random() - 0.5) * 0.05;
                
                // Convert to Int16 range with proper scaling
                audioData[i] = Math.max(-32768, Math.min(32767, Math.round(signal * 20000)));
            }
            
            // Validate audio data
            const minVal = Math.min(...audioData);
            const maxVal = Math.max(...audioData);
            const avgVal = audioData.reduce((a, b) => a + b, 0) / audioData.length;
            const rms = Math.sqrt(audioData.reduce((sum, val) => sum + val * val, 0) / audioData.length);
            
            console.log(`Generated ${language} audio: ${samples} samples, range: ${minVal} to ${maxVal}, avg: ${avgVal.toFixed(2)}, RMS: ${rms.toFixed(2)}`);
            
            return audioData;
        }
        
        // Test cases
        const testCases = [
            {
                language: 'English',
                command: 'Turn it up',
                expectedIntent: true,
                expectedPwmChange: 'increase',
                audioData: generateTestAudio(1.2, 'english'),
                languageCode: 'en-US'
            },
            {
                language: 'Spanish', 
                command: 'MÃ¡s fuerte',
                expectedIntent: true,
                expectedPwmChange: 'increase',
                audioData: generateTestAudio(1.3, 'spanish'),
                languageCode: 'es-ES'
            },
            {
                language: 'Chinese',
                command: 'åŠ å¼º',
                expectedIntent: true,
                expectedPwmChange: 'increase',
                audioData: generateTestAudio(1.1, 'chinese'),
                languageCode: 'zh-CN'
            }
        ];
        
        // Real API call
        async function callRealApi(audioBuffer, currentPwm, msgHis, languageCode, testCase) {
            log(`ğŸ”„ Calling real API for ${testCase.language} audio...`);
            log(`   - Audio buffer: ${audioBuffer.length} samples`);
            log(`   - Duration: ${(audioBuffer.length / 16000).toFixed(2)}s`);
            log(`   - Current PWM: ${currentPwm}`);
            log(`   - Language code: ${languageCode}`);
            
            try {
                log(`   ğŸ“¡ Making API request to: ${API_URL}`);
                
                const requestBody = {
                    msgHis: msgHis,
                    audioData: audioBuffer, // Int16Array format for direct audio API
                    currentPwm: currentPwm
                };
                
                log(`   ğŸ“¦ Request payload size: ${JSON.stringify(requestBody).length} bytes`);
                log(`   ğŸ” Request structure: msgHis=${requestBody.msgHis.length}, audioData=${requestBody.audioData.length}, currentPwm=${requestBody.currentPwm}`);
                
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(requestBody)
                });
                
                log(`   ğŸ“Š API Response Status: ${response.status} ${response.statusText}`);
                
                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`API error ${response.status}: ${errorText}`);
                }
                
                const result = await response.json();
                
                log(`   âœ… API call successful`, 'success');
                log(`   ğŸ“„ Full response: ${JSON.stringify(result, null, 2)}`);
                log(`   ğŸ¤ Transcription: "${result.transcription || 'N/A'}"`);
                log(`   ğŸ¤– Response: "${result.response || 'N/A'}"`);
                log(`   âš¡ PWM: ${result.previousPwm || currentPwm} â†’ ${result.newPwmValue || currentPwm}`);
                log(`   ğŸ¯ Intent detected: ${result.intentDetected || false}`);
                log(`   ğŸ“ˆ Confidence: ${result.confidence ? (result.confidence * 100).toFixed(1) + '%' : 'N/A'}`);
                log(`   ğŸŒ Detected language: ${result.detectedLanguage || 'N/A'}`);
                
                return result;
                
            } catch (error) {
                log(`   âŒ API call failed: ${error.message}`, 'error');
                throw error;
            }
        }
        
        // Test single language
        async function testSingleLanguage(languageFilter) {
            const testCase = testCases.find(tc => tc.language.toLowerCase() === languageFilter);
            if (!testCase) {
                log(`âŒ Test case not found for language: ${languageFilter}`, 'error');
                return;
            }
            
            setButtonsEnabled(false);
            
            log(`\n${'='.repeat(50)}`);
            log(`ğŸ§ª Testing ${testCase.language} Command: "${testCase.command}"`);
            log(`${'='.repeat(50)}`);
            
            const currentPwm = 100;
            const messageHistory = [];
            
            try {
                const result = await callRealApi(
                    testCase.audioData,
                    currentPwm,
                    messageHistory,
                    testCase.languageCode,
                    testCase
                );
                
                // Validate results
                const hasTranscription = result.transcription && result.transcription.length > 0;
                const hasResponse = result.response && result.response.length > 0;
                const pwmIncreased = result.newPwmValue > result.previousPwm;
                const intentMatches = result.intentDetected === testCase.expectedIntent;
                const pwmChangeMatches = (testCase.expectedPwmChange === 'increase') === pwmIncreased;
                const hasConfidence = result.confidence !== undefined && result.confidence > 0;
                const overallSuccess = result.success && hasTranscription && intentMatches && pwmChangeMatches;
                
                log(`\nğŸ“‹ Validation Results:`);
                log(`   - API success: ${result.success ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - Has transcription: ${hasTranscription ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - Has response: ${hasResponse ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - Has confidence: ${hasConfidence ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - Intent detection: ${intentMatches ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - PWM change direction: ${pwmChangeMatches ? 'âœ… PASS' : 'âŒ FAIL'}`);
                log(`   - Overall result: ${overallSuccess ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, overallSuccess ? 'success' : 'error');
                
            } catch (error) {
                log(`âŒ Test failed: ${error.message}`, 'error');
            } finally {
                setButtonsEnabled(true);
            }
        }
        
        // Test with real microphone input
        async function testWithMicrophone() {
            setButtonsEnabled(false);
            
            log(`\nğŸ¤ Testing with Real Microphone...`);
            
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                log(`âœ… Microphone access granted`, 'success');
                log(`ğŸ™ï¸ Please say a motor control command in any language:`, 'info');
                log(`   ğŸ‡ºğŸ‡¸ English: "turn it up" / "make it stronger"`);
                log(`   ğŸ‡ªğŸ‡¸ Spanish: "mÃ¡s fuerte" / "aumenta la potencia"`);
                log(`   ğŸ‡¨ğŸ‡³ Chinese: "åŠ å¼º" / "å¢åŠ åŠŸç‡"`);
                log(`   ğŸŒ Or any motor control command in your language...`);
                
                // Create audio context and recorder
                const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(stream);
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                const audioData = [];
                let recording = true;
                
                processor.onaudioprocess = function(e) {
                    if (recording) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        // Convert to Int16Array
                        for (let i = 0; i < inputData.length; i++) {
                            const sample = Math.max(-1, Math.min(1, inputData[i]));
                            audioData.push(Math.round(sample * 32767));
                        }
                    }
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                // Countdown for user
                let countdown = 3;
                const countdownInterval = setInterval(() => {
                    if (countdown > 0) {
                        log(`ğŸ™ï¸ Recording in ${countdown}...`);
                        countdown--;
                    } else {
                        log(`ğŸ”´ Recording now! Speak your command...`, 'success');
                        clearInterval(countdownInterval);
                    }
                }, 1000);
                
                // Record for 4 seconds (1 second countdown + 3 seconds recording)
                setTimeout(async () => {
                    recording = false;
                    stream.getTracks().forEach(track => track.stop());
                    processor.disconnect();
                    source.disconnect();
                    
                    log(`ğŸµ Recorded ${audioData.length} samples (${(audioData.length / 16000).toFixed(2)}s)`);
                    
                    // Test with recorded audio using auto language detection
                    try {
                        const result = await callRealApi(
                            audioData,
                            100,
                            [],
                            'auto', // Use auto-detection instead of hardcoded language
                            { language: 'Auto-detected', command: 'Real microphone input' }
                        );
                        
                        log(`\nğŸ“‹ Real Microphone Test Results:`);
                        log(`   - Success: ${result.success ? 'âœ…' : 'âŒ'}`);
                        log(`   - Transcription: "${result.transcription || 'None'}"`);
                        log(`   - Detected Language: ${result.detectedLanguage || 'Unknown'}`);
                        log(`   - Confidence: ${result.confidence ? (result.confidence * 100).toFixed(1) + '%' : 'N/A'}`);
                        log(`   - Intent Detected: ${result.intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM Change: ${result.previousPwm || 100} â†’ ${result.newPwmValue || 100}`);
                        log(`   - Response: "${result.response || 'None'}"`);
                        
                        // Validate results
                        const hasTranscription = result.transcription && result.transcription.length > 0;
                        const hasLanguageDetection = result.detectedLanguage && result.detectedLanguage.length > 0;
                        const intentDetected = result.intentDetected;
                        const pwmChanged = result.newPwmValue !== (result.previousPwm || 100);
                        const overallSuccess = result.success && hasTranscription && intentDetected;
                        
                        log(`\nğŸ“Š Microphone Test Validation:`);
                        log(`   - Has transcription: ${hasTranscription ? 'âœ…' : 'âŒ'}`);
                        log(`   - Language detected: ${hasLanguageDetection ? 'âœ…' : 'âŒ'}`);
                        log(`   - Intent recognized: ${intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM changed: ${pwmChanged ? 'âœ…' : 'âŒ'}`);
                        log(`   - Overall result: ${overallSuccess ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, 
                            overallSuccess ? 'success' : 'error');
                        
                        if (!overallSuccess && hasTranscription) {
                            log(`\nğŸ’¡ Tip: Try saying clearer motor control commands like:`, 'warning');
                            log(`   ğŸ‡ºğŸ‡¸ "turn it up", "make it stronger", "increase power"`);
                            log(`   ğŸ‡ªğŸ‡¸ "mÃ¡s fuerte", "aumenta la potencia"`);
                            log(`   ğŸ‡¨ğŸ‡³ "åŠ å¼º", "å¢åŠ åŠŸç‡", "å¼€å¤§ä¸€ç‚¹"`);
                        }
                        
                    } catch (error) {
                        log(`âŒ Real microphone test failed: ${error.message}`, 'error');
                    }
                    
                    setButtonsEnabled(true);
                }, 4000); // 1 second countdown + 3 seconds recording
                
            } catch (error) {
                log(`âŒ Microphone access failed: ${error.message}`, 'error');
                setButtonsEnabled(true);
            }
        }
        
        // Test LLM processing only (bypass speech recognition)
        async function testLLMOnly() {
            setButtonsEnabled(false);
            
            log(`\nğŸ¤– Testing LLM Processing Only...`);
            log(`This bypasses speech recognition and tests the motor control logic directly.`);
            
            const testCases = [
                { transcript: 'turn it up', language: 'English', expected: 'increase' },
                { transcript: 'mÃ¡s fuerte', language: 'Spanish', expected: 'increase' },
                { transcript: 'åŠ å¼º', language: 'Chinese', expected: 'increase' }
            ];
            
            for (const testCase of testCases) {
                try {
                    log(`\nğŸ§ª Testing "${testCase.transcript}" (${testCase.language})`);
                    
                    const testRequest = {
                        msgHis: [],
                        testMode: true, // Enable test mode
                        testTranscript: testCase.transcript, // Provide transcript directly
                        currentPwm: 100,
                        encoding: 'LINEAR16',
                        sampleRateHertz: 16000,
                        languageCode: testCase.language === 'English' ? 'en-US' : 
                                     testCase.language === 'Spanish' ? 'es-ES' : 'zh-CN'
                    };
                    
                    log(`ğŸ“¡ Sending test request with transcript: "${testCase.transcript}"`);
                    
                    const response = await fetch(API_URL, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(testRequest)
                    });
                    
                    log(`ğŸ“Š Response Status: ${response.status} ${response.statusText}`);
                    
                    if (response.ok) {
                        const result = await response.json();
                        log(`âœ… LLM test successful!`, 'success');
                        log(`ğŸ¤ Transcript: "${result.transcription}"`);
                        log(`ğŸ¤– Response: "${result.response}"`);
                        log(`âš¡ PWM: ${result.previousPwm} â†’ ${result.newPwmValue}`);
                        log(`ğŸ¯ Intent detected: ${result.intentDetected}`);
                        log(`ğŸ“ˆ Confidence: ${(result.confidence * 100).toFixed(1)}%`);
                        
                        // Validate results
                        const pwmIncreased = result.newPwmValue > result.previousPwm;
                        const intentDetected = result.intentDetected;
                        const hasResponse = result.response && result.response.length > 0;
                        
                        log(`ğŸ“‹ Validation:`);
                        log(`   - Intent detected: ${intentDetected ? 'âœ…' : 'âŒ'}`);
                        log(`   - PWM increased: ${pwmIncreased ? 'âœ…' : 'âŒ'}`);
                        log(`   - Has response: ${hasResponse ? 'âœ…' : 'âŒ'}`);
                        log(`   - Overall: ${(intentDetected && pwmIncreased && hasResponse) ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, 
                            (intentDetected && pwmIncreased && hasResponse) ? 'success' : 'error');
                        
                    } else {
                        const errorText = await response.text();
                        log(`âŒ LLM test failed: ${errorText}`, 'error');
                    }
                    
                } catch (error) {
                    log(`âŒ LLM test error: ${error.message}`, 'error');
                }
                
                // Small delay between tests
                await new Promise(resolve => setTimeout(resolve, 500));
            }
            
            log(`\nğŸ LLM-only testing completed!`, 'success');
            setButtonsEnabled(true);
        }
        
        // Run all tests
        async function runRealApiTests() {
            setButtonsEnabled(false);
            
            log('ğŸš€ Starting Real API Tests with Multi-language Motor Commands\n');
            log('=' * 60);
            
            let currentPwm = 100;
            let messageHistory = [];
            let testResults = [];
            
            log(`ğŸ“Š Initial state: PWM = ${currentPwm}, Message history = ${messageHistory.length} items\n`);
            
            for (let i = 0; i < testCases.length; i++) {
                const testCase = testCases[i];
                
                log(`\n${'='.repeat(50)}`);
                log(`ğŸ§ª TEST ${i + 1}: ${testCase.language} Command`);
                log(`${'='.repeat(50)}`);
                log(`Command: "${testCase.command}"`);
                log(`Expected: ${testCase.expectedPwmChange} motor power`);
                log(`Audio samples: ${testCase.audioData.length}`);
                log(`Duration: ${(testCase.audioData.length / 16000).toFixed(2)}s`);
                
                try {
                    const result = await callRealApi(
                        testCase.audioData,
                        currentPwm,
                        messageHistory,
                        testCase.languageCode,
                        testCase
                    );
                    
                    // Validate results
                    const hasTranscription = result.transcription && result.transcription.length > 0;
                    const hasResponse = result.response && result.response.length > 0;
                    const pwmIncreased = result.newPwmValue > result.previousPwm;
                    const intentMatches = result.intentDetected === testCase.expectedIntent;
                    const pwmChangeMatches = (testCase.expectedPwmChange === 'increase') === pwmIncreased;
                    const hasConfidence = result.confidence !== undefined && result.confidence > 0;
                    const overallSuccess = result.success && hasTranscription && intentMatches && pwmChangeMatches;
                    
                    log(`\nğŸ“‹ Validation Results:`);
                    log(`   - API success: ${result.success ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - Has transcription: ${hasTranscription ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - Has response: ${hasResponse ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - Has confidence: ${hasConfidence ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - Intent detection: ${intentMatches ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - PWM change direction: ${pwmChangeMatches ? 'âœ… PASS' : 'âŒ FAIL'}`);
                    log(`   - Overall result: ${overallSuccess ? 'âœ… SUCCESS' : 'âŒ FAILED'}`, overallSuccess ? 'success' : 'error');
                    
                    // Update state for next test
                    currentPwm = result.newPwmValue || currentPwm;
                    messageHistory = result.msgHis || messageHistory;
                    
                    testResults.push({
                        testCase: testCase,
                        result: result,
                        validation: {
                            hasTranscription,
                            hasResponse,
                            intentMatches,
                            pwmChangeMatches,
                            hasConfidence,
                            overall: overallSuccess
                        }
                    });
                    
                } catch (error) {
                    log(`âŒ Test ${i + 1} failed with error: ${error.message}`, 'error');
                    testResults.push({
                        testCase: testCase,
                        error: error.message,
                        validation: { overall: false }
                    });
                }
                
                // Add delay between tests
                await new Promise(resolve => setTimeout(resolve, 1000));
            }
            
            // Final summary
            log(`\n${'='.repeat(60)}`);
            log(`ğŸ“Š FINAL REAL API TEST SUMMARY`);
            log(`${'='.repeat(60)}`);
            
            const passedTests = testResults.filter(r => r.validation && r.validation.overall).length;
            const totalTests = testResults.length;
            
            log(`Tests passed: ${passedTests}/${totalTests}`, passedTests === totalTests ? 'success' : 'warning');
            log(`Success rate: ${((passedTests / totalTests) * 100).toFixed(1)}%`);
            log(`Final PWM value: ${currentPwm}`);
            log(`Message history length: ${messageHistory.length}`);
            
            testResults.forEach((result, index) => {
                const status = result.validation && result.validation.overall ? 'âœ…' : 'âŒ';
                const type = result.validation && result.validation.overall ? 'success' : 'error';
                log(`${status} Test ${index + 1} (${result.testCase.language}): ${result.testCase.command}`, type);
            });
            
            log(`\nğŸ”§ Real API Pipeline Verification:`);
            log(`   âœ… Audio data generation: Working`);
            log(`   âœ… Int16Array transmission: Working`);
            log(`   ${passedTests > 0 ? 'âœ…' : 'âŒ'} Real Google Speech-to-Text API: ${passedTests > 0 ? 'Working' : 'Check API'}`);
            log(`   ${passedTests > 0 ? 'âœ…' : 'âŒ'} Real Google Gemini LLM API: ${passedTests > 0 ? 'Working' : 'Check API'}`);
            log(`   âœ… PWM control logic: Working`);
            log(`   ${passedTests === totalTests ? 'âœ…' : 'âš ï¸'} Multi-language support: ${passedTests === totalTests ? 'Working' : 'Partial'}`);
            
            log(`\nğŸ Real API test completed!`, 'success');
            log(`ğŸ’¡ ${passedTests === totalTests ? 'All systems operational!' : 'Some issues detected - check logs above.'}`, passedTests === totalTests ? 'success' : 'warning');
            
            // Show stats
            showTestStats(testResults, passedTests, totalTests, currentPwm, messageHistory.length);
            
            setButtonsEnabled(true);
        }
        
        function showTestStats(results, passed, total, finalPwm, historyLength) {
            const statsDiv = document.getElementById('test-stats');
            const statsContent = document.getElementById('stats-content');
            
            statsContent.innerHTML = `
                <div>âœ… Tests Passed: ${passed}/${total}</div>
                <div>ğŸ“ˆ Success Rate: ${((passed / total) * 100).toFixed(1)}%</div>
                <div>âš¡ Final PWM: ${finalPwm}</div>
                <div>ğŸ’¬ Message History: ${historyLength} items</div>
                <div>ğŸŒ Languages Tested: English, Spanish, Chinese</div>
                <div>ğŸ¯ Real APIs: Google Speech-to-Text + Gemini LLM</div>
                <div>ğŸ”— Endpoint: directaudiotopwm-qveg3gkwxa-ew.a.run.app</div>
            `;
            
            statsDiv.style.display = 'block';
        }
        
        // Initialize
        log('ğŸ§ª Real API Test Runner Initialized', 'success');
        log('Ready to test with direct audio processing via Gemini 2.0!');
        log('âš ï¸  Note: This will make real API calls and may incur costs.');
    </script>
</body>
</html>